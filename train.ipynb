{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054f3008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zlib import crc32\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbf62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(csv_path='2023-01-27_17_10_influxdb_data.csv'):\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        filtered_data = data.drop([\"result\", \"table\", \"_start\", \"_stop\", \"_measurement\", \"key\", \"Unnamed: 0\"], axis=1)\n",
    "        filtered_data[\"_field\"].value_counts()\n",
    "        # parse keys into seperate dataframes\n",
    "        buzz_df = pd.DataFrame([element for element in filtered_data.to_numpy() if element[2] == \"buzz\"])\n",
    "        fill_df = pd.DataFrame([element for element in filtered_data.to_numpy() if element[2] == \"fill\"])\n",
    "        stol_df = pd.DataFrame([element for element in filtered_data.to_numpy() if element[2] == \"stol\"])\n",
    "        #add values for machine learning\n",
    "        past_values_30m = [filtered_data[\"_value\"].loc[[i[0]-131]][i[0]-131] for i in enumerate(filtered_data.to_numpy()) if i[0]>45326+131 and i[0]<90653]\n",
    "        for i in range(0,131):\n",
    "            past_values_30m.insert(0, -1)\n",
    "        past_values_2h = [filtered_data[\"_value\"].loc[[i[0]-545]][i[0]-545] for i in enumerate(filtered_data.to_numpy()) if i[0]>45326+545 and i[0]<90653]\n",
    "        for i in range(0,545):\n",
    "            past_values_2h.insert(0, -1)\n",
    "        past_values_4h = [filtered_data[\"_value\"].loc[[i[0]-1091]][i[0]-1091] for i in enumerate(filtered_data.to_numpy()) if i[0]>45326+1091 and i[0]<90653]\n",
    "        for i in range(0,1091):\n",
    "            past_values_4h.insert(0, -1)\n",
    "        #add result column for ml\n",
    "        future_values = [filtered_data[\"_value\"].loc[[i[0]+1091]][i[0]+1091] for i in enumerate(filtered_data.to_numpy()) if i[0]>45326 and i[0]<89561 and i[1][2] == \"fill\"]\n",
    "        for i in range(0,1092):\n",
    "            future_values.append(-1)\n",
    "        # merge dataframe for use in spark\n",
    "        temp_all_rows = pd.merge(fill_df, buzz_df, how='inner', on=0)\n",
    "        temp_all_rows[\"fill_future_4h\"] = future_values\n",
    "        temp_all_rows[\"fill_past_30m\"] = past_values_30m\n",
    "        temp_all_rows[\"fill_past_2h\"] = past_values_2h\n",
    "        temp_all_rows[\"fill_past_4h\"] = past_values_4h\n",
    "        all_rows = pd.merge(temp_all_rows, stol_df, how='inner', on=0)\n",
    "        renamed_rows = all_rows.rename(columns={'1_x': 'fill', '1_y': 'buzz', 1: 'stol', 0: 'tod'})\n",
    "        pandas_df = renamed_rows.drop(['2_x', '2_y', 2], axis=1)\n",
    "        pandas_df[\"tod\"] = [datetime.strptime(row, '%Y-%m-%dT%H:%M:%S') for row in [row.split('Z')[0] for row in [row.split('.')[0] for row in pandas_df[\"tod\"]]]]\n",
    "        #parse date into seperate columns\n",
    "        pandas_df[\"year\"] = [date.year for date in pandas_df[\"tod\"]]\n",
    "        pandas_df[\"month\"] = [date.month for date in pandas_df[\"tod\"]]\n",
    "        pandas_df[\"day\"] = [date.day for date in pandas_df[\"tod\"]]\n",
    "        pandas_df[\"hour\"] = [date.hour for date in pandas_df[\"tod\"]]\n",
    "        pandas_df[\"minute\"] = [date.minute for date in pandas_df[\"tod\"]]\n",
    "        pandas_df[\"second\"] = [date.second for date in pandas_df[\"tod\"]]\n",
    "        #clean data\n",
    "        pandas_df = pandas_df.drop(['tod'], axis=1)\n",
    "        pandas_df = pandas_df.drop(pandas_df[pandas_df.fill_future_4h < 0].index)\n",
    "        pandas_df = pandas_df.drop(pandas_df[pandas_df.fill_past_4h < 0].index)\n",
    "        sparkDF=spark.createDataFrame(pandas_df)\n",
    "        feature_cols = ['year', 'month', 'day', 'hour', 'minute', 'second', 'fill', 'buzz', 'stol', 'fill_past_30m', 'fill_past_2h', 'fill_past_4h']\n",
    "        vect_assembler = VectorAssembler(inputCols = feature_cols, outputCol = \"features\")\n",
    "        data_w_features = vect_assembler.transform(sparkDF)\n",
    "        # specify output column\n",
    "        data_for_training = data_w_features.select('features', 'fill_future_4h')\n",
    "        # train-test-split\n",
    "        train_dataset, test_dataset = data_for_training.randomSplit([0.7, 0.3])\n",
    "        LinReg = LinearRegression(featuresCol = \"features\", labelCol = \"fill_future_4h\")\n",
    "        model = LinReg.fit(train_dataset)\n",
    "        model.write().overwrite().save('./models/')\n",
    "        print(\"trained and written model in the models folder!\")\n",
    "    except:\n",
    "        print(\"Error while training the model! Training is still in development and does not work for other datasets than in the Repository!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b084d618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained and written model in the models folder!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221ab43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "v_": [
   "dGlsbHRyaXMyMA==",
   "dGlsbHRyaXMyMA=="
  ],
  "vscode": {
   "interpreter": {
    "hash": "b8c929f1c7b5a7b7d05dbe3bcbfebf7850db0ed0607b815f99d0534cfe000cb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
